**Techinal R1**
Starts with short description about the company and the role & responsibilities. 
1. Tell me about yourself and your journey.
2. Enquired about my work in previous company, cross questioned and asked to explain why and how.
3. Importance of accuracy, recall in ML.
4. What is Regression trade-off.
5. Explain Bias and Variance. 
6. How to handle underfitting and overfitting?
7. What are regularization techniques?
8. Explain Vanishing Gradient. Why does it happen?
9. What is self supervized learning?
10. Training LLM, is it supervised Learning or Unsupervised Learning? How?
11. Masking in NLP.
12. Explain XGBoost.
13. What is few-shot learning in LLMs.
14. How to implement RAG?
    
These are some main points, the interviewer crossed questioned on these topics and drilled here and there to check conceptual knowledge.

---
**Technical R2**
1. Gave general scenarios, asked me to model these problem and what metrics would I use to test them. 
2. Discussion about metrics and forecasting. 
3. Questions related to my resume and projects, steps I took, modelling etc.
4. How feature engineering works, main steps?
5. Questions about XGBoost and Random Forest
6. Gini Index, formulae, importance, etc.
7. Gradient Descent and Back propagation.
8. Questions with how to detect protien folding pattern, how to build same structure as given using blocks. Provided contextual information too.
9. Genetic Algorithms and Brute force algorithms. 
10. Questions over Llama 7B model, characterstics, etc.
11. Quantization of LLMs.
12. Fine tuning LLMs, getting structured output from LLMs.
13. Encoder, Decoder models, working.
14. Gaurdrailing and RedTeaming for LLMs. 
---

**Take Home Assignment**
- To be completed within 7 days, with ppt and report. 

**Technical Interview**
- Project presentation
- Cross Questioning on the steps, methodologies.
- Further Questions on LLMs
  - Query Expansion?
  - Lost in the middle reference
  - How to choose the right chunk size?
  - Graph DB for RAG
  - When to use RAG, when to fine-tune the LLM
  - Why LLM so sucessfull in predicting the next word ?

